{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import os.path as path\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from yacs.config import CfgNode as CN\n",
    "from typing import Any, Optional, Union, List\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "\n",
    "# load csv\n",
    "# DATA_PATH = \"../data/custom/tmp_dataset_w_bldtype_area.csv\"\n",
    "DATA_PATH = \"../data/custom/tmp_dataset_with_interest_col_modified.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - Single Series\n",
    "class TSSingleDataset(Dataset):\n",
    "    def __init__(self, data, x_cols, input_steps, output_steps, scaler=None):\n",
    "        self.dataframe= data if isinstance(data, pd.DataFrame) else pd.DataFrame(data)\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "\n",
    "        # setup scaler\n",
    "        if scaler is not None:\n",
    "            self.scaler = scaler\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(self.dataframe[x_cols])\n",
    "\n",
    "        # prepare data\n",
    "        self.data = torch.tensor(\n",
    "            self.scaler.transform(self.dataframe[x_cols]), \n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.n_features = self.data.shape[1]    # feature-dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.input_steps - self.output_steps + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # x : (input_steps, n_features)\n",
    "        x = self.data[idx:idx + self.input_steps,:]\n",
    "        y = self.data[idx + self.input_steps:idx + self.input_steps + self.output_steps,:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - Multi Series\n",
    "class TSMultiDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data: Union[List[pd.DataFrame], pd.DataFrame], \n",
    "                 x_cols, \n",
    "                 input_steps, \n",
    "                 output_steps):\n",
    "        \n",
    "        # data : list of dataframe or single dataframe (all same shape / time series length)\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.df_list = [data] if isinstance(data, pd.DataFrame) else data\n",
    "        self.df_combined = pd.concat(self.df_list, axis=0)\n",
    "        self.series_length = self.df_list[0].shape[0] - self.input_steps - self.output_steps + 1   # length of each series\n",
    "        \n",
    "        # setup scaler & data\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(self.df_combined[x_cols])\n",
    "\n",
    "        # dim : (N_series, N_timesteps, N_features)\n",
    "        self.data = torch.tensor(\n",
    "            np.asarray([self.scaler.transform(df[x_cols]) for df in self.df_list]),\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.n_features = self.data.shape[2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.series_length * len(self.df_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_1 = idx // self.series_length\n",
    "        idx_2 = idx % self.series_length\n",
    "\n",
    "        # shape : X - (input_steps, N_features), Y - (output_steps, N_features)\n",
    "        x = self.data[idx_1,idx_2:idx_2 + self.input_steps,:]\n",
    "        y = self.data[idx_1,idx_2 + self.input_steps:idx_2 + self.input_steps + self.output_steps,:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModule - Single Series\n",
    "class TSSingleDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path: str, emd_cd: str, input_steps: int, output_steps: int, x_cols: list=None) -> None:\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.emd_cd = emd_cd\n",
    "        self.x_cols = x_cols\n",
    "    \n",
    "    def prepare_data(self) -> None:\n",
    "        df = pd.read_csv(self.data_path, low_memory=False)\n",
    "        df.dropna(how='any',inplace=True)\n",
    "        self.dataframe = df.loc[df['EMD_CD']==self.emd_cd].sort_values(by='STD_YM')\n",
    "        if not self.x_cols is not None:\n",
    "            self.x_cols = list(df.columns[2:])    # exclude index columns\n",
    "        \n",
    "        # Y (vacancy_rate) column must be at the front\n",
    "        if self.x_cols[0] != 'vacancy_rate':\n",
    "            self.x_cols.insert(0, self.x_cols.pop(self.x_cols.index('vacancy_rate')))\n",
    "\n",
    "    def setup(self, stage: str=None) -> None:\n",
    "        # (train, val, test) -> (0.6, 0.2, 0.2)\n",
    "        train, test = train_test_split(\n",
    "            self.dataframe,\n",
    "            test_size=0.2,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        train, val = train_test_split(\n",
    "            train,\n",
    "            test_size=0.25,\n",
    "            shuffle=False\n",
    "        )\n",
    "        # scaler is set from train-set only\n",
    "        self.train = TSSingleDataset(train, self.x_cols, self.input_steps, self.output_steps)\n",
    "        self.scaler = self.train.scaler\n",
    "        self.n_features = self.train.n_features\n",
    "        self.validation = TSSingleDataset(val, self.x_cols, self.input_steps, self.output_steps, scaler=self.scaler)\n",
    "        self.test = TSSingleDataset(test, self.x_cols, self.input_steps, self.output_steps, scaler=self.scaler)\n",
    "    \n",
    "    def train_dataloader(self, batch_size: int=8):\n",
    "        return DataLoader(self.train, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self, batch_size: int=8):\n",
    "        return DataLoader(self.validation, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self, batch_size: int=8):\n",
    "        return DataLoader(self.test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModule - Multi Series\n",
    "class TSMultiDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path: str, input_steps: int, output_steps: int, x_cols: list=None) -> None:\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.x_cols = x_cols\n",
    "    \n",
    "    def prepare_data(self) -> None:\n",
    "        df = pd.read_csv(self.data_path, low_memory=False)\n",
    "        df.dropna(how='any',inplace=True)\n",
    "        df.set_index('EMD_CD',inplace=True)\n",
    "        self.df_list = [df.loc[emd].reset_index(drop=False).sort_values(by='STD_YM') for emd in df.index.unique()]\n",
    "        if not self.x_cols:\n",
    "            self.x_cols = list(self.df_list[0].columns[2:])    # exclude index columns\n",
    "\n",
    "        # Y (vacancy_rate) column must be at the front\n",
    "        if self.x_cols[0] != 'vacancy_rate':\n",
    "            self.x_cols.insert(0, self.x_cols.pop(self.x_cols.index('vacancy_rate')))\n",
    "\n",
    "    def setup(self, stage: str=None) -> None:\n",
    "        # (train, val, test) -> (0.6, 0.2, 0.2)\n",
    "        splits = [train_test_split(df, test_size=0.2, shuffle=False) for df in self.df_list]\n",
    "        trains_t, tests = [x[0] for x in splits], [x[1] for x in splits]\n",
    "        splits = [train_test_split(df, test_size=0.25, shuffle=False) for df in trains_t]\n",
    "        trains, vals = [x[0] for x in splits], [x[1] for x in splits]\n",
    "\n",
    "        # scaler is set from train-set only\n",
    "        self.train = TSMultiDataset(trains, x_cols=self.x_cols, input_steps=self.input_steps, output_steps=self.output_steps)\n",
    "        self.scaler = self.train.scaler\n",
    "        self.n_features = self.train.n_features\n",
    "        self.validation = TSMultiDataset(vals, x_cols=self.x_cols, input_steps=self.input_steps, output_steps=self.output_steps)\n",
    "        self.test = TSMultiDataset(tests, x_cols=self.x_cols, input_steps=self.input_steps, output_steps=self.output_steps)\n",
    "    \n",
    "    def train_dataloader(self, batch_size: int=8):\n",
    "        return DataLoader(self.train, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self, batch_size: int=8):\n",
    "        return DataLoader(self.validation, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self, batch_size: int=8):\n",
    "        return DataLoader(self.test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LSTMSimple(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, bidirectional=False, scaler=None):\n",
    "        super(LSTMSimple, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.scaler = scaler\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            dropout=0, \n",
    "                            bidirectional=bidirectional, \n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * (2 if self.bidirectional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden state\n",
    "        h_0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # forward lstm & fcn\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning module\n",
    "class LSTMSimpleLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model=None, cfg=None, scaler=None):\n",
    "        super(LSTMSimpleLightningModule, self).__init__()\n",
    "        assert (model is not None) or (cfg is not None)\n",
    "        \n",
    "        # init by either model or CfgNode\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        else:\n",
    "            assert scaler is not None, \"Dataset Scaler must be provided with CfgNode\"\n",
    "            self.model = LSTMSimple(\n",
    "                input_size=cfg['input_size'],\n",
    "                output_size=cfg['output_size'],\n",
    "                hidden_size=cfg['hidden_size'],\n",
    "                num_layers=cfg['num_layers'],\n",
    "                bidirectional=cfg['LSTM_bidirectional'] if 'LSTM_bidirectional' in cfg else False,\n",
    "                scaler=scaler,\n",
    "            )\n",
    "        \n",
    "        self.scaler = self.model.scaler\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.test_predictions = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                'train_mse_loss': loss,\n",
    "            },\n",
    "            on_epoch=True,\n",
    "            # on_step=False,\n",
    "        )\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                'val_mse_loss': loss,\n",
    "            },\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "        )\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.test_predictions.append((y_hat, y))\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                'test_mse_loss': loss,\n",
    "            },\n",
    "            on_step=True,\n",
    "        )\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        preds = [x[0] for x in self.test_predictions]\n",
    "        gt = [x[1] for x in self.test_predictions]\n",
    "        print(len(preds), len(gt))\n",
    "        print(preds[0].shape, gt[0].shape)\n",
    "        return None\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer,\n",
    "                mode=\"min\",\n",
    "                patience=2,\n",
    "                min_lr=1e-5,\n",
    "            ),\n",
    "            \"monitor\": \"val_mse_loss\",\n",
    "        }\n",
    "        return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning-Datamodule : Multi\n",
    "data_module = TSMultiDataModule(\n",
    "    data_path=DATA_PATH,\n",
    "    input_steps=3,\n",
    "    output_steps=1,\n",
    ")\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightningModule\n",
    "cfg = CN(dict(\n",
    "    # input/output dimensions\n",
    "    input_size=data_module.n_features,\n",
    "    output_size=data_module.n_features * data_module.output_steps,\n",
    "    # hyperparams\n",
    "    num_layers=1,\n",
    "    hidden_size=32,\n",
    "    LSTM_bidirectional=True,\n",
    "))\n",
    "l_model = LSTMSimpleLightningModule(\n",
    "    cfg=cfg,\n",
    "    scaler=data_module.scaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/11 17:00:03 INFO mlflow.utils.credentials: Successfully connected to MLflow hosted tracking server! Host: https://community.cloud.databricks.com.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type       | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model     | LSTMSimple | 16.4 K | train\n",
      "1 | criterion | MSELoss    | 0      | train\n",
      "-------------------------------------------------\n",
      "16.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.4 K    Total params\n",
      "0.066     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bffe0ee5e54a55a6ec4645ba65f1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cwwojin/anaconda3/envs/ml2/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (29) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55204555a1b4359b5016cc2269688df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce001cded75b4039a681e93d0643fe89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f7d0f6514548a097f43ddffe204237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee47679e0f34842bd0406128e13d71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae17a0964c794017bdb2466233503d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d666eb417844dde948e05eb628f719d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803e060241364e929a114a8695c51ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0db92bf2e24ef4a39bc9d8c9487e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76b6db541ae45fc9c7c13f40783c57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8efb6b4e07455a951c0ee2187a9bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df371efe7b3d4a1db53830ab7bf3fd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4c78df53c6491f94d79c6130504f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3cec2aecea4b70b9a0cb55a2366787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d62515c775c4c2f80b43bfabe61f09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0597e88a4c1440368e26530412948ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c423191eaa3845dda2e219ec8e44f901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2de2b24d384d95b7e9423237991c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642776f175674f9689c26759c8b0b2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9ca8be51304870ac3d82de6abd9c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec450749de934ed2b76b5ecb0c1d9a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bfc6bbb09b4f1e8dc05ece173834f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6633a64a79994bb289ab1724d9611f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cdd5d8a2c1446894b14c071b188318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab953278e054f2db5e453745c13b6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d64b334e978491e82e83e0b873f06be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e0e078abc248ba9c5c3bcc802f1f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b21f59d6de4098895f42d20b40cc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train with MLflow\n",
    "mlflow.login()\n",
    "name = \"Compas_LSTM\"\n",
    "timestamp = datetime.strftime(datetime.now(),\"%Y-%m-%d_%H-%m-%s\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=25, \n",
    "    devices=\"auto\",\n",
    "    # callbacks=[],\n",
    "    logger=MLFlowLogger(\n",
    "        experiment_name=f\"/Users/cwwojin@gmail.com/{name}\",\n",
    "        run_name=f\"run_{timestamp}\",\n",
    "        tracking_uri=\"databricks\",\n",
    "        log_model=True,\n",
    "    ),\n",
    "    # log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=1,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=l_model, \n",
    "    datamodule=data_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f12bddae8d4000adc13be508740737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n",
      "torch.Size([8, 24]) torch.Size([8, 24])\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   test_mse_loss_epoch      0.5889014601707458\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_mse_loss_epoch': 0.5889014601707458}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(\n",
    "    model=l_model,\n",
    "    datamodule=data_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import pytorch_lightning as pl\n",
    "# import torch.optim as optim\n",
    "# import mlflow.pytorch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Create dummy data\n",
    "# data = np.random.randn(48, 7)\n",
    "# df = pd.DataFrame(data, columns=[f'feature_{i}' for i in range(7)])\n",
    "\n",
    "# # Standardize data\n",
    "# scaler = StandardScaler()\n",
    "# scaled_data = scaler.fit_transform(df)\n",
    "# data_tensor = torch.tensor(scaled_data, dtype=torch.float32)\n",
    "\n",
    "# # Dataset\n",
    "# class TimeSeriesDataset(Dataset):\n",
    "#     def __init__(self, data, input_steps, output_steps):\n",
    "#         self.data = data\n",
    "#         self.input_steps = input_steps\n",
    "#         self.output_steps = output_steps\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data) - self.input_steps - self.output_steps + 1\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         x = self.data[idx:idx + self.input_steps]\n",
    "#         y = self.data[idx + self.input_steps:idx + self.input_steps + self.output_steps]\n",
    "#         return x, y\n",
    "\n",
    "# input_steps = 36\n",
    "# output_steps = 12\n",
    "# dataset = TimeSeriesDataset(data_tensor, input_steps, output_steps)\n",
    "# dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# # Model\n",
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "#         c_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "#         out, _ = self.lstm(x, (h_0, c_0))\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         return out\n",
    "\n",
    "# input_size = 7\n",
    "# hidden_size = 64\n",
    "# num_layers = 2\n",
    "# output_size = 7 * output_steps\n",
    "# model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# # Lightning module\n",
    "# class TimeSeriesLightningModule(pl.LightningModule):\n",
    "#     def __init__(self, model):\n",
    "#         super(TimeSeriesLightningModule, self).__init__()\n",
    "#         self.model = model\n",
    "#         self.criterion = nn.MSELoss()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         y = y.view(y.size(0), -1)\n",
    "#         y_hat = self(x)\n",
    "#         loss = self.criterion(y_hat, y)\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "#         return optimizer\n",
    "\n",
    "# lightning_model = TimeSeriesLightningModule(model)\n",
    "\n",
    "# # Train with MLflow\n",
    "# mlflow.pytorch.autolog()\n",
    "# trainer = pl.Trainer(max_epochs=50, gpus=1 if torch.cuda.is_available() else 0)\n",
    "# with mlflow.start_run() as run:\n",
    "#     trainer.fit(lightning_model, dataloader)\n",
    "#     mlflow.pytorch.log_model(lightning_model.model, \"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
