{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import os.path as path\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from yacs.config import CfgNode as CN\n",
    "from typing import Any, Optional, Union, List\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "\n",
    "# load csv\n",
    "# DATA_PATH = \"../data/custom/tmp_dataset_w_bldtype_area.csv\"\n",
    "DATA_PATH = \"../data/custom/tmp_dataset_with_interest_col_modified.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - Single Series\n",
    "class TSSingleDataset(Dataset):\n",
    "    def __init__(self, data, x_cols, input_steps, output_steps, scaler=None):\n",
    "        self.dataframe= data if isinstance(data, pd.DataFrame) else pd.DataFrame(data)\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "\n",
    "        # setup scaler\n",
    "        if scaler is not None:\n",
    "            self.scaler = scaler\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(self.dataframe[x_cols])\n",
    "\n",
    "        # prepare data\n",
    "        self.data = torch.tensor(\n",
    "            self.scaler.transform(self.dataframe[x_cols]), \n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.n_features = self.data.shape[1]    # feature-dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.input_steps - self.output_steps + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # x : (input_steps, n_features)\n",
    "        x = self.data[idx:idx + self.input_steps,:]\n",
    "        y = self.data[idx + self.input_steps:idx + self.input_steps + self.output_steps,:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - Multi Series\n",
    "class TSMultiDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data: Union[List[pd.DataFrame], pd.DataFrame], \n",
    "                 x_cols, \n",
    "                 input_steps, \n",
    "                 output_steps):\n",
    "        \n",
    "        # data : list of dataframe or single dataframe (all same shape / time series length)\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.df_list = [data] if isinstance(data, pd.DataFrame) else data\n",
    "        self.df_combined = pd.concat(self.df_list, axis=0)\n",
    "        self.series_length = self.df_list[0].shape[0] - self.input_steps - self.output_steps + 1   # length of each series\n",
    "        \n",
    "        # setup scaler & data\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(self.df_combined[x_cols])\n",
    "\n",
    "        # dim : (N_series, N_timesteps, N_features)\n",
    "        self.data = torch.tensor(\n",
    "            np.asarray([self.scaler.transform(df[x_cols]) for df in self.df_list]),\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.n_features = self.data.shape[2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.series_length * len(self.df_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_1 = idx // self.series_length\n",
    "        idx_2 = idx % self.series_length\n",
    "\n",
    "        # shape : X - (input_steps, N_features), Y - (output_steps, N_features)\n",
    "        x = self.data[idx_1,idx_2:idx_2 + self.input_steps,:]\n",
    "        y = self.data[idx_1,idx_2 + self.input_steps:idx_2 + self.input_steps + self.output_steps,:]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModule - Single Series\n",
    "class TSSingleDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path: str, emd_cd: str, input_steps: int, output_steps: int, x_cols: list=None) -> None:\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.emd_cd = emd_cd\n",
    "        self.x_cols = x_cols\n",
    "    \n",
    "    def prepare_data(self) -> None:\n",
    "        df = pd.read_csv(self.data_path, low_memory=False)\n",
    "        df.dropna(how='any',inplace=True)\n",
    "        self.dataframe = df.loc[df['EMD_CD']==self.emd_cd].sort_values(by='STD_YM')\n",
    "        if not self.x_cols is not None:\n",
    "            self.x_cols = list(df.columns[2:])    # exclude index columns\n",
    "        \n",
    "        # Y (vacancy_rate) column must be at the front\n",
    "        if self.x_cols[0] != 'vacancy_rate':\n",
    "            self.x_cols.insert(0, self.x_cols.pop(self.x_cols.index('vacancy_rate')))\n",
    "\n",
    "    def setup(self, stage: str=None) -> None:\n",
    "        # (train, val, test) -> (0.6, 0.2, 0.2)\n",
    "        train, test = train_test_split(\n",
    "            self.dataframe,\n",
    "            test_size=0.2,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        train, val = train_test_split(\n",
    "            train,\n",
    "            test_size=0.25,\n",
    "            shuffle=False\n",
    "        )\n",
    "        # scaler is set from train-set only\n",
    "        self.train = TSSingleDataset(train, self.x_cols, self.input_steps, self.output_steps)\n",
    "        self.scaler = self.train.scaler\n",
    "        self.n_features = self.train.n_features\n",
    "        self.validation = TSSingleDataset(val, self.x_cols, self.input_steps, self.output_steps, scaler=self.scaler)\n",
    "        self.test = TSSingleDataset(test, self.x_cols, self.input_steps, self.output_steps, scaler=self.scaler)\n",
    "    \n",
    "    def train_dataloader(self, batch_size: int=8):\n",
    "        return DataLoader(self.train, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self, batch_size: int=8):\n",
    "        return DataLoader(self.validation, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self, batch_size: int=8):\n",
    "        return DataLoader(self.test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModule - Multi Series\n",
    "class TSMultiDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path: str, input_steps: int, output_steps: int, x_cols: list=None) -> None:\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.x_cols = x_cols\n",
    "    \n",
    "    def prepare_data(self) -> None:\n",
    "        df = pd.read_csv(self.data_path, low_memory=False)\n",
    "        df.dropna(how='any',inplace=True)\n",
    "        df.set_index('EMD_CD',inplace=True)\n",
    "        self.df_list = [df.loc[emd].reset_index(drop=False).sort_values(by='STD_YM') for emd in df.index.unique()]\n",
    "        if not self.x_cols:\n",
    "            self.x_cols = list(self.df_list[0].columns[2:])    # exclude index columns\n",
    "\n",
    "        # Y (vacancy_rate) column must be at the front\n",
    "        if self.x_cols[0] != 'vacancy_rate':\n",
    "            self.x_cols.insert(0, self.x_cols.pop(self.x_cols.index('vacancy_rate')))\n",
    "\n",
    "    def setup(self, stage: str=None) -> None:\n",
    "        # (train, val, test) -> (0.6, 0.2, 0.2)\n",
    "        splits = [train_test_split(df, test_size=0.2, shuffle=False) for df in self.df_list]\n",
    "        trains_t, tests = [x[0] for x in splits], [x[1] for x in splits]\n",
    "        splits = [train_test_split(df, test_size=0.25, shuffle=False) for df in trains_t]\n",
    "        trains, vals = [x[0] for x in splits], [x[1] for x in splits]\n",
    "\n",
    "        # scaler is set from train-set only\n",
    "        self.train = TSMultiDataset(trains, x_cols=self.x_cols, input_steps=self.input_steps, output_steps=self.output_steps)\n",
    "        self.scaler = self.train.scaler\n",
    "        self.n_features = self.train.n_features\n",
    "        self.validation = TSMultiDataset(vals, x_cols=self.x_cols, input_steps=self.input_steps, output_steps=self.output_steps)\n",
    "        self.test = TSMultiDataset(tests, x_cols=self.x_cols, input_steps=self.input_steps, output_steps=self.output_steps)\n",
    "    \n",
    "    def train_dataloader(self, batch_size: int=8):\n",
    "        return DataLoader(self.train, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self, batch_size: int=8):\n",
    "        return DataLoader(self.validation, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self, batch_size: int=8):\n",
    "        return DataLoader(self.test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LSTMSimple(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, bidirectional=False, scaler=None):\n",
    "        super(LSTMSimple, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.scaler = scaler\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            dropout=0, \n",
    "                            bidirectional=bidirectional, \n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * (2 if self.bidirectional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden state\n",
    "        h_0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1), x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # forward lstm & fcn\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning module\n",
    "class LSTMSimpleLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model=None, cfg=None, scaler=None):\n",
    "        super(LSTMSimpleLightningModule, self).__init__()\n",
    "        assert (model is not None) or (cfg is not None)\n",
    "        \n",
    "        # init by either model or CfgNode\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        else:\n",
    "            assert scaler is not None, \"Dataset Scaler must be provided with CfgNode\"\n",
    "            self.model = LSTMSimple(\n",
    "                input_size=cfg['input_size'],\n",
    "                output_size=cfg['output_size'],\n",
    "                hidden_size=cfg['hidden_size'],\n",
    "                num_layers=cfg['num_layers'],\n",
    "                bidirectional=cfg['LSTM_bidirectional'] if 'LSTM_bidirectional' in cfg else False,\n",
    "                scaler=scaler,\n",
    "            )\n",
    "        \n",
    "        self.scaler = self.model.scaler\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.test_predictions = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                'train_mse_loss': loss,\n",
    "            },\n",
    "            on_epoch=True,\n",
    "            # on_step=False,\n",
    "        )\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                'val_mse_loss': loss,\n",
    "            },\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "        )\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.test_predictions.append((y_hat, y))\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                'test_mse_loss': loss,\n",
    "            },\n",
    "            on_step=True,\n",
    "        )\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        preds = [x[0] for x in self.test_predictions]\n",
    "        gt = [x[1] for x in self.test_predictions]\n",
    "        print(len(preds), len(gt))\n",
    "        print(preds[0].shape, gt[0].shape)\n",
    "        return None\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer,\n",
    "                mode=\"min\",\n",
    "                patience=2,\n",
    "                min_lr=1e-5,\n",
    "            ),\n",
    "            \"monitor\": \"val_mse_loss\",\n",
    "        }\n",
    "        return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning-Datamodule : Multi\n",
    "data_module = TSMultiDataModule(\n",
    "    data_path=DATA_PATH,\n",
    "    input_steps=3,\n",
    "    output_steps=1,\n",
    ")\n",
    "\n",
    "data_module.prepare_data()\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightningModule\n",
    "cfg = CN(dict(\n",
    "    # input/output dimensions\n",
    "    input_size=data_module.n_features,\n",
    "    output_size=data_module.n_features * data_module.output_steps,\n",
    "    # hyperparams\n",
    "    num_layers=1,\n",
    "    hidden_size=32,\n",
    "    LSTM_bidirectional=True,\n",
    "))\n",
    "l_model = LSTMSimpleLightningModule(\n",
    "    cfg=cfg,\n",
    "    scaler=data_module.scaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with MLflow\n",
    "mlflow.login()\n",
    "name = \"Compas_LSTM\"\n",
    "workspace_path = \"/Users/cwwojin@gmail.com\" # EDIT HERE\n",
    "\n",
    "timestamp = datetime.strftime(datetime.now(),\"%Y-%m-%d_%H-%m-%s\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=25, \n",
    "    devices=\"auto\",\n",
    "    # callbacks=[],\n",
    "    logger=MLFlowLogger(\n",
    "        experiment_name=f\"{workspace_path}/{name}\",\n",
    "        run_name=f\"run_{timestamp}\",\n",
    "        tracking_uri=\"databricks\",\n",
    "        log_model=True,\n",
    "    ),\n",
    "    # log_every_n_steps=1,\n",
    "    check_val_every_n_epoch=1,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=l_model, \n",
    "    datamodule=data_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(\n",
    "    model=l_model,\n",
    "    datamodule=data_module,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
