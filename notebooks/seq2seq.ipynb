{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import os.path as path\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from yacs.config import CfgNode as CN\n",
    "from typing import Any, Optional, Union, List\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "\n",
    "# load csv\n",
    "DATA_PATH = \"../data/custom/tmp_dataset_with_interest_col_modified.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - Single Series\n",
    "class TSSingleDataset(Dataset):\n",
    "    def __init__(self, data, x_cols, input_steps, output_steps, scaler=None):\n",
    "        self.dataframe = data if isinstance(data, pd.DataFrame) else pd.DataFrame(data)\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "\n",
    "        # setup scaler\n",
    "        if scaler is not None:\n",
    "            self.scaler = scaler\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(self.dataframe[x_cols])\n",
    "\n",
    "        # prepare data\n",
    "        self.data = torch.tensor(\n",
    "            self.scaler.transform(self.dataframe[x_cols]),\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.n_features = self.data.shape[1]  # feature-dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.input_steps - self.output_steps + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # x : (input_steps, n_features)\n",
    "        x = self.data[idx : idx + self.input_steps, :]\n",
    "        y = self.data[\n",
    "            idx + self.input_steps : idx + self.input_steps + self.output_steps, :\n",
    "        ]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset - Multi Series\n",
    "class TSMultiDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Union[List[pd.DataFrame], pd.DataFrame],\n",
    "        x_cols,\n",
    "        input_steps,\n",
    "        output_steps,\n",
    "        scaler=None,\n",
    "    ):\n",
    "\n",
    "        # data : list of dataframe or single dataframe (all same shape / time series length)\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.df_list = [data] if isinstance(data, pd.DataFrame) else data\n",
    "        self.df_combined = pd.concat(self.df_list, axis=0)\n",
    "        self.series_length = (\n",
    "            self.df_list[0].shape[0] - self.input_steps - self.output_steps + 1\n",
    "        )  # length of each series\n",
    "\n",
    "        # setup scaler\n",
    "        if scaler is not None:\n",
    "            self.scaler = scaler\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(self.df_combined[x_cols])\n",
    "\n",
    "        # dim : (N_series, N_timesteps, N_features)\n",
    "        self.data = torch.tensor(\n",
    "            np.asarray([self.scaler.transform(df[x_cols]) for df in self.df_list]),\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        self.n_features = self.data.shape[2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.series_length * len(self.df_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_1 = idx // self.series_length\n",
    "        idx_2 = idx % self.series_length\n",
    "\n",
    "        # shape : X - (input_steps, N_features), Y - (output_steps, N_features)\n",
    "        x = self.data[idx_1, idx_2 : idx_2 + self.input_steps, :]\n",
    "        y = self.data[\n",
    "            idx_1,\n",
    "            idx_2 + self.input_steps : idx_2 + self.input_steps + self.output_steps,\n",
    "            :,\n",
    "        ]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModule - Single Series\n",
    "class TSSingleDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        emd_cd: str,\n",
    "        input_steps: int,\n",
    "        output_steps: int,\n",
    "        test_size: int = 12,\n",
    "        val_size: int = 12,\n",
    "        batch_size: int = 8,\n",
    "        x_cols: list = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.emd_cd = emd_cd\n",
    "        self.x_cols = x_cols\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        df = pd.read_csv(self.data_path, low_memory=False)\n",
    "        df.dropna(how=\"any\", inplace=True)\n",
    "        self.dataframe = df.loc[df[\"EMD_CD\"] == self.emd_cd].sort_values(by=\"STD_YM\")\n",
    "        if not self.x_cols is not None:\n",
    "            self.x_cols = list(df.columns[2:])  # exclude index columns\n",
    "\n",
    "        # Y (vacancy_rate) column must be at the front\n",
    "        if self.x_cols[0] != \"vacancy_rate\":\n",
    "            self.x_cols.insert(0, self.x_cols.pop(self.x_cols.index(\"vacancy_rate\")))\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        # (train, val, test) -> (ANY, 12, 12)\n",
    "        train, test = train_test_split(\n",
    "            self.dataframe,\n",
    "            test_size=self.test_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        train, val = train_test_split(train, test_size=self.test_size, shuffle=False)\n",
    "        # scaler is set from train-set only\n",
    "        self.train = TSSingleDataset(\n",
    "            train, self.x_cols, self.input_steps, self.output_steps\n",
    "        )\n",
    "        self.scaler = self.train.scaler\n",
    "        self.n_features = self.train.n_features\n",
    "        self.validation = TSSingleDataset(\n",
    "            val, self.x_cols, self.input_steps, self.output_steps, scaler=self.scaler\n",
    "        )\n",
    "        self.test = TSSingleDataset(\n",
    "            test, self.x_cols, self.input_steps, self.output_steps, scaler=self.scaler\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validation, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataModule - Multi Series\n",
    "class TSMultiDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        input_steps: int,\n",
    "        output_steps: int,\n",
    "        test_size: int = 12,\n",
    "        val_size: int = 12,\n",
    "        batch_size: int = 8,\n",
    "        x_cols: list = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.x_cols = x_cols\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        df = pd.read_csv(self.data_path, low_memory=False)\n",
    "        df.dropna(how=\"any\", inplace=True)\n",
    "        df.set_index(\"EMD_CD\", inplace=True)\n",
    "        self.df_list = [\n",
    "            df.loc[emd].reset_index(drop=False).sort_values(by=\"STD_YM\")\n",
    "            for emd in df.index.unique()\n",
    "        ]\n",
    "        if not self.x_cols:\n",
    "            self.x_cols = list(self.df_list[0].columns[2:])  # exclude index columns\n",
    "\n",
    "        # Y (vacancy_rate) column must be at the front\n",
    "        if self.x_cols[0] != \"vacancy_rate\":\n",
    "            self.x_cols.insert(0, self.x_cols.pop(self.x_cols.index(\"vacancy_rate\")))\n",
    "\n",
    "    def setup(self, stage: str = None) -> None:\n",
    "        # (train, val, test) -> (ANY, 12, 12)\n",
    "        splits = [\n",
    "            train_test_split(df, test_size=self.test_size, shuffle=False)\n",
    "            for df in self.df_list\n",
    "        ]\n",
    "        trains_t, tests = [x[0] for x in splits], [x[1] for x in splits]\n",
    "        splits = [\n",
    "            train_test_split(df, test_size=self.val_size, shuffle=False)\n",
    "            for df in trains_t\n",
    "        ]\n",
    "        trains, vals = [x[0] for x in splits], [x[1] for x in splits]\n",
    "\n",
    "        # scaler is set from train-set only\n",
    "        self.train = TSMultiDataset(\n",
    "            trains,\n",
    "            x_cols=self.x_cols,\n",
    "            input_steps=self.input_steps,\n",
    "            output_steps=self.output_steps,\n",
    "        )\n",
    "        self.scaler = self.train.scaler\n",
    "        self.n_features = self.train.n_features\n",
    "        self.validation = TSMultiDataset(\n",
    "            vals,\n",
    "            x_cols=self.x_cols,\n",
    "            input_steps=self.input_steps,\n",
    "            output_steps=self.output_steps,\n",
    "            scaler=self.scaler,\n",
    "        )\n",
    "        self.test = TSMultiDataset(\n",
    "            tests,\n",
    "            x_cols=self.x_cols,\n",
    "            input_steps=self.input_steps,\n",
    "            output_steps=self.output_steps,\n",
    "            scaler=self.scaler,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.validation, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class LSTMSimple(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        output_steps,\n",
    "        bidirectional=False,\n",
    "        scaler=None,\n",
    "    ):\n",
    "        super(LSTMSimple, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_steps = output_steps\n",
    "        self.output_size = output_steps * input_size  # input_size == N_FEATURES\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.scaler = scaler\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_size * (2 if self.bidirectional else 1), self.output_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden state\n",
    "        h_0 = torch.zeros(\n",
    "            self.num_layers * (2 if self.bidirectional else 1),\n",
    "            x.size(0),\n",
    "            self.hidden_size,\n",
    "        ).to(x.device)\n",
    "        c_0 = torch.zeros(\n",
    "            self.num_layers * (2 if self.bidirectional else 1),\n",
    "            x.size(0),\n",
    "            self.hidden_size,\n",
    "        ).to(x.device)\n",
    "\n",
    "        # forward lstm & fcn\n",
    "        out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning module\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class LSTMSimpleLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model=None, cfg=None, scaler=None):\n",
    "        super(LSTMSimpleLightningModule, self).__init__()\n",
    "        assert (model is not None) or (cfg is not None)\n",
    "\n",
    "        # init by either model or CfgNode\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        else:\n",
    "            assert scaler is not None, \"Dataset Scaler must be provided with CfgNode\"\n",
    "            self.model = LSTMSimple(\n",
    "                input_size=cfg[\"input_size\"],\n",
    "                output_steps=cfg[\"output_steps\"],\n",
    "                hidden_size=cfg[\"hidden_size\"],\n",
    "                num_layers=cfg[\"num_layers\"],\n",
    "                bidirectional=cfg[\"bidirectional\"] if \"bidirectional\" in cfg else False,\n",
    "                scaler=scaler,\n",
    "            )\n",
    "\n",
    "        self.scaler = self.model.scaler\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.test_predictions = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_mse_loss\": loss,\n",
    "            },\n",
    "            on_epoch=True,\n",
    "        )\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"val_mse_loss\": loss,\n",
    "            },\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "        )\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.test_predictions.append((y_hat, y))\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"test_mse_loss\": loss,\n",
    "            },\n",
    "            on_step=True,\n",
    "        )\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, _ = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # Reshape y_hat as time series (B, output_steps, input_size)\n",
    "        batch_size = x.size(0)\n",
    "        preds = y_hat.view(batch_size, self.model.output_steps, self.model.input_size)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.scheduler = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer,\n",
    "                mode=\"min\",\n",
    "                patience=2,\n",
    "                min_lr=1e-5,\n",
    "            ),\n",
    "            \"monitor\": \"val_mse_loss\",\n",
    "        }\n",
    "        return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**configs**\n",
    "1. dataset : `data_path, input_steps, output_steps, test_size, val_size, x_cols`\n",
    "2. model-type : `model_type == enum(['lstm','gru','cnn'])`\n",
    "3. model-lstm : `num_layers, hidden_size, bidirectional(bool)`\n",
    "4. model-gru : \n",
    "5. model-cnn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yacs.config import CfgNode as CN\n",
    "\n",
    "cfg = CN()\n",
    "cfg.MODEL_TYPE = \"lstm\"\n",
    "cfg.LSTM = CN()\n",
    "cfg.DATA_PATH = \"./data/custom/tmp_dataset_with_interest_col_modified.csv\"\n",
    "cfg.INPUT_STEPS = 3\n",
    "cfg.OUTPUT_STEPS = 1\n",
    "cfg.TEST_SIZE = 12\n",
    "cfg.VAL_SIZE = 12\n",
    "cfg.X_COLS = None\n",
    "\n",
    "cfg.N_EPOCHS = 50\n",
    "cfg.BATCH_SIZE = 8\n",
    "\n",
    "cfg.LSTM.NUM_LAYERS = 1\n",
    "cfg.LSTM.HIDDEN_SIZE = 32\n",
    "cfg.LSTM.BIDIRECTIONAL = False\n",
    "\n",
    "cfg.MLFLOW_TRACKING_URI = \"databricks\"\n",
    "cfg.DATABRICKS_WORKSPACE = \"/Users/cwwojin@gmail.com\"\n",
    "cfg.EXPERIMENT_NAME = f\"Compas_{cfg.MODEL_TYPE.upper()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning-Datamodule : Multi\n",
    "data_module = TSMultiDataModule(\n",
    "    data_path=cfg.DATA_PATH,\n",
    "    input_steps=cfg.INPUT_STEPS,\n",
    "    output_steps=cfg.OUTPUT_STEPS,\n",
    "    test_size=cfg.TEST_SIZE,\n",
    "    val_size=cfg.VAL_SIZE,\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    x_cols=cfg.X_COLS,\n",
    ")\n",
    "data_module.prepare_data()\n",
    "data_module.setup()\n",
    "\n",
    "# lightningModule\n",
    "l_model = LSTMSimpleLightningModule(\n",
    "    cfg=dict(\n",
    "        input_size=data_module.n_features,\n",
    "        output_steps=cfg.OUTPUT_STEPS,\n",
    "        num_layers=cfg.LSTM.NUM_LAYERS,\n",
    "        hidden_size=cfg.LSTM.HIDDEN_SIZE,\n",
    "        bidirectional=cfg.LSTM.BIDIRECTIONAL,\n",
    "    ),\n",
    "    scaler=data_module.scaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with MLflow\n",
    "mlflow.login()\n",
    "timestamp = datetime.strftime(datetime.now(), \"%Y-%m-%d_%H-%m-%s\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=cfg.N_EPOCHS,\n",
    "    devices=\"auto\",\n",
    "    logger=MLFlowLogger(\n",
    "        experiment_name=f\"{cfg.DATABRICKS_WORKSPACE}/{cfg.EXPERIMENT_NAME}\",\n",
    "        run_name=f\"run_{timestamp}\",\n",
    "        tracking_uri=cfg.MLFLOW_TRACKING_URI,\n",
    "        log_model=True,\n",
    "    ),\n",
    "    check_val_every_n_epoch=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model=l_model,\n",
    "    datamodule=data_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(\n",
    "    model=l_model,\n",
    "    datamodule=data_module,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to TorchScript\n",
    "- Export model -> `.pt file of scripted-model`\n",
    "- Export data scaler -> `.pkl file using SKLearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export lightningmodule\n",
    "# l_model.to_torchscript(\"./models/lightning_module.pt\")\n",
    "\n",
    "# # import & check forward\n",
    "# imported_model = torch.jit.load(\"./models/lightning_module.pt\")\n",
    "# imported_model(torch.Tensor(np.random.randn(8, 3, 24))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export pytorch nn.Module\n",
    "# scripted_model = torch.jit.script(l_model.model)\n",
    "# torch.jit.save(scripted_model, \"./models/torch_module.pt\")\n",
    "\n",
    "# # import & check forward\n",
    "# imported_model = torch.jit.load(\"./models/torch_module.pt\")\n",
    "# imported_model(torch.Tensor(np.random.randn(8, 3, 24))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference & graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path as path\n",
    "from compas.inference import ForecastModel\n",
    "\n",
    "model = ForecastModel(\n",
    "    model_path=\"../.saved_models/gru_custom_GRU_uni_in_6_out_1_2024-07-22_09:36:32\"\n",
    ")\n",
    "df = pd.read_csv(\"../data/custom/dataset_v1.1.csv\")\n",
    "df = df.dropna()\n",
    "emd_target = list(df[\"EMD_CD\"].unique())\n",
    "df_emd = pd.read_csv(\"../data/custom/data_01.pnu_gid_emd_map.csv\", low_memory=False)\n",
    "emd_map = dict(zip(df_emd[\"EMD_CD\"], df_emd[\"EMD_NM\"]))\n",
    "\n",
    "results = {}\n",
    "for emd_cd in emd_target:\n",
    "    df_sample = df.loc[df[\"EMD_CD\"] == emd_cd].sort_values(by=\"STD_YM\")\n",
    "    # predict last 12 months\n",
    "    df_gt = df_sample.iloc[-12:, :].set_index(\"STD_YM\")\n",
    "    df_out = model.forecast(df_sample.iloc[:-12, :], steps=12)\n",
    "    results[emd_cd] = (df_gt[\"vacancy_rate\"], df_out[\"vacancy_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc(\"font\", family=\"AppleGothic\")\n",
    "\n",
    "fig, ax = plt.subplots(5, 2, figsize=(36, 24))\n",
    "for i, (k, v) in enumerate(results.items()):\n",
    "    v[0].plot(\n",
    "        ax=ax[i // 2, i % 2],\n",
    "        # ylim=(0, 0.6),\n",
    "        title=emd_map[k],\n",
    "        legend=True,\n",
    "        label=\"gt\",\n",
    "        ylabel=\"vacancy rate\",\n",
    "    )\n",
    "    v[1].plot(\n",
    "        ax=ax[i // 2, i % 2],\n",
    "        # ylim=(0, 0.6),\n",
    "        title=emd_map[k],\n",
    "        legend=True,\n",
    "        label=\"pred\",\n",
    "        ylabel=\"vacancy rate\",\n",
    "    )\n",
    "# fig.savefig(\"fig.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference - Sejong City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path as path\n",
    "from compas.inference import ForecastModel\n",
    "\n",
    "model = ForecastModel(\"../.saved_models/GRU_uni_in_3_out_1_2024-07-22_13:44:18\")\n",
    "df = pd.read_csv(\"../data/custom/dataset_v1.1.csv\")\n",
    "emd_target = list(df[\"EMD_CD\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. inference w/ sejong-city total dataset\n",
    "df_sejong = df.copy()\n",
    "df_sejong['vacancy_rate'] = df_sejong['vacancy_rate'] * df_sejong['bld_tot_area']\n",
    "df_sejong = df_sejong.groupby('STD_YM').agg({\n",
    "    'vacancy_rate':'sum',\n",
    "    'move_pop':'sum',\n",
    "    'area_pop':'sum',\n",
    "    'service_type_count':'max', \n",
    "    'biz_opens':'sum', \n",
    "    'biz_closures':'sum', \n",
    "    'bld_tot_area':'sum',\n",
    "    'bld_area_small':'sum', \n",
    "    'bld_area_midlarge':'sum', \n",
    "    'bld_area_complex':'sum',\n",
    "    'maxgrid_lat':'mean', \n",
    "    'maxgrid_lon':'mean', \n",
    "    'call_rate':'first', \n",
    "    'novel_balance_COFIX':'first',\n",
    "    'bld_loan_complex':'first', \n",
    "    'novel_trade_COFIX':'first', \n",
    "    'avg_comp_stock':'first',\n",
    "    'balance_COFIX':'first', \n",
    "    'avg_treasury_10yrs':'first', \n",
    "    'bld_loan_small':'first',\n",
    "    'avg_treasury_5yrs':'first', \n",
    "    'CPI':'first', \n",
    "    'bld_loan_midlarge':'first', \n",
    "    'avg_treasury_3yrs':'first',\n",
    "    'CD_91':'first', \n",
    "    'standard_interest':'first',\n",
    "}).reset_index().sort_values('STD_YM')\n",
    "df_sejong['vacancy_rate'] = df_sejong['vacancy_rate'] / df_sejong['bld_tot_area']\n",
    "df_sejong_out = model.forecast(df_sejong, steps=19)[['vacancy_rate']]\n",
    "\n",
    "# 2. inference per-EMD, post process sum\n",
    "results = []\n",
    "for emd_cd in emd_target:\n",
    "    df_sample = df.loc[df[\"EMD_CD\"] == emd_cd].sort_values(by=\"STD_YM\")\n",
    "    df_out = model.forecast(df_sample, steps=19)\n",
    "    results.append(df_out[[\"vacancy_rate\",\"bld_tot_area\"]])\n",
    "df_out = pd.concat(results,axis=0).reset_index().rename(columns={'index':'STD_YM'})\n",
    "df_out['vacancy_rate'] = df_out['vacancy_rate'] * df_out['bld_tot_area']\n",
    "df_out = df_out.groupby('STD_YM').agg('sum')\n",
    "df_out['vacancy_rate'] = df_out['vacancy_rate'] / df_out['bld_tot_area']\n",
    "df_out = df_out[['vacancy_rate']]\n",
    "\n",
    "result_df = pd.concat([\n",
    "    df_out.rename(columns={'vacancy_rate':'vac_EMD'}), \n",
    "    df_sejong_out.rename(columns={'vacancy_rate':'vac_Sejong'})],\n",
    "    axis=1,\n",
    ")\n",
    "result_df.to_csv(\"./inference_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.line(\n",
    "    pd.concat([\n",
    "        pd.concat([df_sejong.set_index('STD_YM')['vacancy_rate'],result_df['vac_EMD']]).rename('EMD-wise'),\n",
    "        pd.concat([df_sejong.set_index('STD_YM')['vacancy_rate'],result_df['vac_Sejong']]).rename('City-wise')],\n",
    "        axis=1,\n",
    "    ),\n",
    "    title='Vacancy Rate Prediction',\n",
    "    labels={'index': 'Time', 'value': 'Vacancy Rate'},\n",
    ")\n",
    "fig.add_vline(x='2024-05', line_dash='dash', line_color='green')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance - Permutation importance\n",
    "- feature importance = **loss difference when shuffling a single feature / column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path as path\n",
    "import plotly.express as px\n",
    "\n",
    "DATA_PATH = \"../data/.saved_models/LSTM_uni_in_3_out_1_2024-07-23_06:45:07\"\n",
    "\n",
    "df = pd.read_csv(path.join(DATA_PATH, \"feature_importance.csv\"))\n",
    "fig = px.bar(\n",
    "    df,\n",
    "    x='feature_name',\n",
    "    y='feature_importance',\n",
    "    title='Feature Importance (Relative)',\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
